{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc9ec83-d140-4352-9b1a-5cf9fa6da96e",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "- dataset: https://archive.ics.uci.edu/dataset/53/iris\n",
    "- task: use the Iris dataset (binary classification case, so you can limit it to only two classes, e.g., Setosa vs. Versicolor) and perform logistic regression with 30% test data to classify the flowers into two categories based on their features (sepal and petal lengths and widths)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e349ea2-5ec9-4bc4-903c-1ddf41e22941",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "- dataset: https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic\n",
    "- task: use the Breast Cancer dataset to predict cancer diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4877c2-ccda-4f30-b310-3fdef59e7197",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "- dataset: returns.csv\n",
    "- task: use the features Stock_1 and Stock_2 to classify whether Stock_3 went up on that day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98688782-bfa3-41e0-9b60-d4f591a22991",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "- dataset: simpson_paradox.csv\n",
    "- task 1: use linear regression to find a model for the col1 and col2 features\n",
    "- task 2: try combining linear regression and logistic regression to improve your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8becb93f-f22f-496e-8d55-c2840f882054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = Pipeline([\\n    (\"scaler\", StandardScaler()),\\n    (\"rbf_transform\", rbf_feature),  # your transform here\\n    (\"log_reg\", LogisticRegression())\\n])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For exercises 5, 6, and 7, your pipeline might look something like:\n",
    "\"\"\"\n",
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"rbf_transform\", rbf_feature),  # your transform here\n",
    "    (\"log_reg\", LogisticRegression())\n",
    "])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb284e-7093-4aa4-9125-5a0ae71fcb02",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "- dataset: ring_data.csv\n",
    "- task 1: find a map phi: R^2 -> R^3 that separates your datapoints\n",
    "- task 2: train a logistic regression (linear model) on your transformed data\n",
    "- task 3: Use scikit-learn PolynomialFeatures(degree=2) and train a logistic regression on the transformed data\n",
    "- task 4: Use scikit-learn RBFSampler(gamma=1.0, n_components=100, random_state=42) and train a logistic regression on the transformed data\n",
    "\n",
    "Measure your classifications using the accuracy metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2112b84b-56c2-41ce-abb9-857e6a73b3e2",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "- dataset: moon_data.csv\n",
    "- task: Use scikit-learn RBFSampler(gamma=1.0, n_components=100, random_state=42) and train a logistic regression on the transformed data\n",
    "\n",
    "Measure your classifications using the accuracy metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b2caf-35a5-4b89-b467-0eac4321f8d8",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "- dataset: spiral_data.csv\n",
    "- task: Use scikit-learn RBFSampler(gamma=1.0, n_components=100, random_state=42) and train a logistic regression on the transformed data\n",
    "\n",
    "Measure your classifications using the accuracy metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0729dab-ede8-4614-9627-7ac15384fb35",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "\n",
    "- task: implement a KernelLinearRegression from scratch. The user should be able to select a kernel function to be used inside the sigmoid activation function. Test it on one of the above dataset and compare with the coresponding exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ee9b2-5ca8-435e-9064-aab37f883eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from scipy.special import expit\n",
    "\n",
    "class KernelLogisticRegression:\n",
    "    def __init__(self, kernel=\"rbf\", gamma=1.0, degree=3, lambda_reg=1e-3, lr=0.1, max_iter=500):\n",
    "        \"\"\"\n",
    "        Kernelized Logistic Regression using Gradient Descent.\n",
    "        :param kernel: \"linear\", \"polynomial\", or \"rbf\"\n",
    "        :param gamma: RBF kernel parameter\n",
    "        :param degree: Polynomial kernel degree\n",
    "        :param lambda_reg: Regularization parameter\n",
    "        :param lr: Learning rate for gradient descent\n",
    "        :param max_iter: Maximum number of iterations\n",
    "        \"\"\"\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma\n",
    "        self.degree = degree\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = None\n",
    "        self.X_train = None\n",
    "\n",
    "    def _compute_kernel(self, X1, X2):\n",
    "        \"\"\" Compute the Kernel Matrix K(X1, X2) \"\"\"\n",
    "        if self.kernel == \"linear\":\n",
    "            return X1 @ X2.T  # K(x, y) = ⟨x, y⟩\n",
    "        \n",
    "        elif self.kernel == \"polynomial\":\n",
    "            return (1 + X1 @ X2.T) ** self.degree  # K(x, y) = (1 + ⟨x, y⟩)^d\n",
    "        \n",
    "        elif self.kernel == \"rbf\":\n",
    "            sq_dists = np.sum(X1**2, axis=1, keepdims=True) - 2 * X1 @ X2.T + np.sum(X2**2, axis=1)\n",
    "            return np.exp(-self.gamma * sq_dists)  # K(x, y) = exp(-γ||x - y||^2)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unknown kernel type. Choose 'linear', 'polynomial', or 'rbf'.\")\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\" Sigmoid function: σ(z) = 1 / (1 + exp(-z)) \"\"\"\n",
    "        return expit(z)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Train Kernel Logistic Regression using Gradient Descent \"\"\"\n",
    "        self.X_train = X\n",
    "        K = self._compute_kernel(X, X)\n",
    "        n = K.shape[0]\n",
    "        \n",
    "        self.alpha = np.zeros(n)\n",
    "        for _ in range(self.max_iter):\n",
    "            y_pred = self.sigmoid(K @ self.alpha)\n",
    "            gradient = K.T @ (y_pred - y) + self.lambda_reg * self.alpha\n",
    "            self.alpha -= self.lr * gradient\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Predict probability estimates using sigmoid activation \"\"\"\n",
    "        K_test = self._compute_kernel(X, self.X_train)\n",
    "        return self.sigmoid(K_test @ self.alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels (0 or 1) \"\"\"\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
